{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://i.imgur.com/eBRPvWB.png)\n",
    "\n",
    "# Practical PyTorch: Generating Shakespeare with a Character-Level RNN\n",
    "\n",
    "[In the RNN classification tutorial](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) we used a RNN to classify text one character at a time. This time we'll generate text one character at a time.\n",
    "\n",
    "```\n",
    "> python generate.py -n 500\n",
    "\n",
    "PAOLTREDN:\n",
    "Let, yil exter shis owrach we so sain, fleas,\n",
    "Be wast the shall deas, puty sonse my sheete.\n",
    "\n",
    "BAUFIO:\n",
    "Sirh carrow out with the knonuot my comest sifard queences\n",
    "O all a man unterd.\n",
    "\n",
    "PROMENSJO:\n",
    "Ay, I to Heron, I sack, againous; bepear, Butch,\n",
    "An as shalp will of that seal think.\n",
    "\n",
    "NUKINUS:\n",
    "And house it to thee word off hee:\n",
    "And thou charrota the son hange of that shall denthand\n",
    "For the say hor you are of I folles muth me?\n",
    "```\n",
    "\n",
    "This one might make you question the series title &mdash; \"is that really practical?\" However, these sorts of generative models form the basis of machine translation, image captioning, question answering and more. See the [Sequence to Sequence Translation tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb) for more on that topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommended Reading\n",
    "\n",
    "I assume you have at least installed PyTorch, know Python, and understand Tensors:\n",
    "\n",
    "* http://pytorch.org/ For installation instructions\n",
    "* [Deep Learning with PyTorch: A 60-minute Blitz](https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb) to get started with PyTorch in general\n",
    "* [jcjohnson's PyTorch examples](https://github.com/jcjohnson/pytorch-examples) for an in depth overview\n",
    "* [Introduction to PyTorch for former Torchies](https://github.com/pytorch/tutorials/blob/master/Introduction%20to%20PyTorch%20for%20former%20Torchies.ipynb) if you are former Lua Torch user\n",
    "\n",
    "It would also be useful to know about RNNs and how they work:\n",
    "\n",
    "* [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) shows a bunch of real life examples\n",
    "* [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) is about LSTMs specifically but also informative about RNNs in general\n",
    "\n",
    "Also see these related tutorials from the series:\n",
    "\n",
    "* [Classifying Names with a Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/char-rnn-classification/char-rnn-classification.ipynb) uses an RNN for classification\n",
    "* [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb) builds on this model to add a category as input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data\n",
    "\n",
    "The file we are using is a plain text file. We turn any potential unicode characters into plain ASCII by using the `unidecode` package (which you can install via `pip` or `conda`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_len = 4373595\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "import string\n",
    "import random\n",
    "import re\n",
    "\n",
    "all_characters = string.printable\n",
    "n_characters = len(all_characters)\n",
    "\n",
    "file = unidecode.unidecode(open('datasets/test.txt').read())\n",
    "val_file = unidecode.unidecode(open('datasets/validation.txt').read())\n",
    "val_len = len(val_file)\n",
    "file_len = len(file)\n",
    "print('file_len =', file_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make inputs out of this big string of data, we will be splitting it into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " acted, there could be no danger of it; and as long as\n",
      "all mention of Bath scenes were avoided, she thought she could behave\n",
      "to him very civilly. In such considerations time passed away, and it was\n",
      "cer\n"
     ]
    }
   ],
   "source": [
    "chunk_len = 200\n",
    "\n",
    "def random_chunk():\n",
    "    start_index = random.randint(0, file_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return file[start_index:end_index]\n",
    "\n",
    "def random_val_chunk():\n",
    "    start_index = random.randint(0, val_len - chunk_len)\n",
    "    end_index = start_index + chunk_len + 1\n",
    "    return val_file[start_index:end_index]\n",
    "\n",
    "print(random_chunk())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Model\n",
    "\n",
    "This model will take as input the character for step $t_{-1}$ and is expected to output the next character $t$. There are three layers - one linear layer that encodes the input character into an internal state, one GRU layer (which may itself have multiple layers) that operates on that internal state and a hidden state, and a decoder layer that outputs the probability distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(RNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = self.encoder(input.view(1, -1))\n",
    "        output, hidden = self.gru(input.view(1, 1, -1), hidden)\n",
    "        output = self.decoder(output.view(1, -1))\n",
    "        return output, hidden\n",
    "\n",
    "    def init_hidden(self):\n",
    "        return Variable(torch.zeros(self.n_layers, 1, self.hidden_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inputs and Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each chunk will be turned into a tensor, specifically a `LongTensor` (used for integer values), by looping through the characters of the string and looking up the index of each character in `all_characters`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11, 12, 39, 40, 41])\n"
     ]
    }
   ],
   "source": [
    "# Turn string into list of longs\n",
    "def char_tensor(string):\n",
    "    tensor = torch.zeros(len(string)).long()\n",
    "    for c in range(len(string)):\n",
    "        tensor[c] = all_characters.index(string[c])\n",
    "    return Variable(tensor)\n",
    "\n",
    "print(char_tensor('abcDEF'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can assemble a pair of input and target tensors for training, from a random chunk. The input will be all characters *up to the last*, and the target will be all characters *from the first*. So if our chunk is \"abc\" the input will correspond to \"ab\" while the target is \"bc\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_training_set():\n",
    "    while True:\n",
    "        chunk = random_chunk()\n",
    "        inp = char_tensor(chunk[:-1])\n",
    "        target = char_tensor(chunk[1:])\n",
    "        if len(inp) != 0 and len(target) !=0:\n",
    "            break\n",
    "    return inp, target\n",
    "\n",
    "def random_validation_set():\n",
    "    while True:\n",
    "        chunk = random_val_chunk()\n",
    "        inp = char_tensor(chunk[:-1])\n",
    "        target = char_tensor(chunk[1:])\n",
    "        if len(inp) != 0 and len(target) !=0:\n",
    "            break\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating\n",
    "\n",
    "To evaluate the network we will feed one character at a time, use the outputs of the network as a probability distribution for the next character, and repeat. To start generation we pass a priming string to start building up the hidden state, from which we then generate one character at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str='A', predict_len=100, temperature=0.8):\n",
    "    hidden = decoder.init_hidden()\n",
    "    prime_input = char_tensor(prime_str)\n",
    "    predicted = prime_str\n",
    "\n",
    "    # Use priming string to \"build up\" hidden state\n",
    "    for p in range(len(prime_str) - 1):\n",
    "        _, hidden = decoder(prime_input[p], hidden)\n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for p in range(predict_len):\n",
    "        output, hidden = decoder(inp, hidden)\n",
    "        \n",
    "        # Sample from the network as a multinomial distribution\n",
    "        output_dist = output.data.view(-1).div(temperature).exp()\n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        # Add predicted character to string and use as next input\n",
    "        predicted_char = all_characters[top_i]\n",
    "        predicted += predicted_char\n",
    "        inp = char_tensor(predicted_char)\n",
    "\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper to print the amount of time passed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "\n",
    "def time_since(since):\n",
    "    s = time.time() - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].flatten())\n",
    "\n",
    "    loss.backward()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item()/ chunk_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we define the training parameters, instantiate the model, and start training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 22s (100 5%) 2.2314]\n",
      "Wh'gre orachery selachas and penttin omad the, mers the the now noum secot bestifty peer aress ang it  \n",
      "\n",
      "[0m 40s (200 10%) 2.7844]\n",
      "Whe spandey\n",
      "and surl; and of and for mbeted of thee for her at colditthe rounksed\n",
      "at wheary fart Ren w \n",
      "\n",
      "[1m 0s (300 15%) 2.2559]\n",
      "Wh the Frary murt be had\n",
      "a war And,  nothing the her a thove chanother shis\n",
      "re, and her common of inte \n",
      "\n",
      "[1m 17s (400 20%) 1.8902]\n",
      "Whe wighto bele gould not whansterst seuced faen every she counten; the hin kent be must of there the  \n",
      "\n",
      "[1m 35s (500 25%) 1.8215]\n",
      "Wh there in imsels her and the my her shich commed an the the nerated for there of penevery to the giv \n",
      "\n",
      "[1m 53s (600 30%) 2.4637]\n",
      "Whathat in.\" Heing sowning have\n",
      "that of this he sall cruch pelessing thimshalleme,\" sarly, had pose it \n",
      "\n",
      "[2m 10s (700 35%) 2.0379]\n",
      "Whed faving have eder.\n",
      "To _iEd. Wectoldles at fathery by it have not haver souch of kne said\n",
      "having a  \n",
      "\n",
      "[2m 27s (800 40%) 1.8810]\n",
      "Whon I give to was as present, and necere, but was eack.\n",
      "\"I the been hee to be cleate, \n",
      "\"if, that spie \n",
      "\n",
      "[2m 44s (900 45%) 2.2130]\n",
      "Wh were has not it intenced at sain feeling mis with sirance quite her grandstred he was hardeds, but  \n",
      "\n",
      "[3m 4s (1000 50%) 1.6609]\n",
      "Whed nevin any comeming to house to plasted to was to had remptance to\n",
      "to saen to be she dispone to wi \n",
      "\n",
      "[3m 22s (1100 55%) 1.9890]\n",
      "Whermunily nistand aparmacas ey was cond it urings, hid in the sister thing\n",
      "have have in the pay, and  \n",
      "\n",
      "[3m 39s (1200 60%) 1.7302]\n",
      "When, would nocliackial and you was what as CiPpat at the have any and with\n",
      "percomed wit lith which \"A \n",
      "\n",
      "[3m 58s (1300 65%) 1.8549]\n",
      "Whtions supperisured him in crear and and meinaly of\n",
      "are were eraid, and would and there is recurd at  \n",
      "\n",
      "[4m 15s (1400 70%) 1.9582]\n",
      "What the for interest confort\n",
      "and a contrabiese, that\n",
      "here was not be had injorned what more some know \n",
      "\n",
      "[4m 32s (1500 75%) 1.6794]\n",
      "Whel, and her be had nom\n",
      "her his see his soner in sees him seementbed her, with the hope, I converises \n",
      "\n",
      "[4m 51s (1600 80%) 1.6570]\n",
      "Wheme it, I cormfordigh at that must have not of him of aman ston, \"I had stad she plite he his disara \n",
      "\n",
      "[5m 10s (1700 85%) 1.7024]\n",
      "Whundly attents much not he there to the\n",
      "which and posether, and being on not sented think comploing,\n",
      " \n",
      "\n",
      "[5m 28s (1800 90%) 1.8176]\n",
      "Whamed, when\n",
      "was uche ham. I chilled the jom do so. The work. I houch would be himprence was an of be  \n",
      "\n",
      "[5m 46s (1900 95%) 2.0862]\n",
      "Whelf from Mariand, with of who well. But Miss Dillight\n",
      "toot aware, and will what offectly whilh as a\n",
      " \n",
      "\n",
      "[6m 4s (2000 100%) 1.7303]\n",
      "Why, and so in ther?\n",
      "\n",
      "\"I perfaine. She ending the his be. The pressift him any it a his attens' fol th \n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 2000\n",
    "print_every = 100\n",
    "plot_every = 10\n",
    "hidden_size = 100\n",
    "n_layers = 1\n",
    "lr = 0.005\n",
    "\n",
    "decoder = RNN(n_characters, hidden_size, n_characters, n_layers)\n",
    "decoder_optimizer = torch.optim.Adam(decoder.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "start = time.time()\n",
    "all_losses = []\n",
    "loss_avg = 0\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    loss = train(*random_training_set())       \n",
    "    loss_avg += loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the Training Losses\n",
    "\n",
    "Plotting the historical loss from all_losses shows the network learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1cf13cea940>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_losses)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating at different \"temperatures\"\n",
    "\n",
    "In the `evaluate` function above, every time a prediction is made the outputs are divided by the \"temperature\" argument passed. Using a higher number makes all actions more equally likely, and thus gives us \"more random\" outputs. Using a lower value (less than 1) makes high probabilities contribute more. As we turn the temperature towards zero we are choosing only the most likely outputs.\n",
    "\n",
    "We can see the effects of this by adjusting the `temperature` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 7s (100 5%) 1.7303]\n",
      "Whould not for a vervening he fatheran to could do her some is a forming, it it all the possead it am  \n",
      "\n",
      "5.299183729580725\n",
      "[0m 13s (200 10%) 1.7303]\n",
      "Wheor\n",
      "mading thood it in the any to quite the persaily a wantle\n",
      "on I am trike to uppersent with down y \n",
      "\n",
      "5.1646159807223855\n",
      "[0m 20s (300 15%) 1.7303]\n",
      "Whelanced resemvoore\n",
      "a resoldite the allatt of mote in the in Mrs. Marst like as I might its\n",
      "was not a \n",
      "\n",
      "7.006435410294428\n",
      "[0m 27s (400 20%) 1.7303]\n",
      "Whes would not poster on her ming to had peasition,\n",
      "on. But I he had not love very in as meed lefte th \n",
      "\n",
      "8.522280940187079\n",
      "[0m 34s (500 25%) 1.7303]\n",
      "Whose alway room an to sankifes or sping as\n",
      "vile Mrs Chuss and mind in think, I can not belies. To not \n",
      "\n",
      "5.520426932792082\n",
      "[0m 41s (600 30%) 1.7303]\n",
      "Wher he his had been you, I\n",
      "are for pay pever it want a though more tem that the never repervaing in t \n",
      "\n",
      "4.772112130094987\n",
      "[0m 48s (700 35%) 1.7303]\n",
      "When who were to his again the a replept love sith with prover the sam see is the\n",
      "see her preseve, als \n",
      "\n",
      "5.9333817566749865\n",
      "[0m 55s (800 40%) 1.7303]\n",
      "When the CiInuct, and wover of more awarns impreave in\n",
      "a very not it, a mane on her up a she vere seal \n",
      "\n",
      "6.681993367341807\n",
      "[1m 3s (900 45%) 1.7303]\n",
      "Wherly and\n",
      "mean a very need as bas no his not like other, you pere and a milution, \"I would esequesard \n",
      "\n",
      "8.550085771880724\n",
      "[1m 9s (1000 50%) 1.7303]\n",
      "Wh to head, the sever imise, in she dorsing possifficed proved sure\n",
      "thing loven she conversant, mad th \n",
      "\n",
      "5.290333818980338\n",
      "[1m 17s (1100 55%) 1.7303]\n",
      "Whes I perepreven this conton\n",
      "the repled not at I shought in the the some very a be sit possible\n",
      "for b \n",
      "\n",
      "4.2755180376184665\n",
      "[1m 23s (1200 60%) 1.7303]\n",
      "Whise I reserved all the\n",
      "indentance a deverpen a the received the do?\" Mr. Evare or to lat it when sho \n",
      "\n",
      "5.566269932986405\n",
      "[1m 30s (1300 65%) 1.7303]\n",
      "Whund's quite impplare into hors, I has\n",
      "in suld a down I was a good me silen to he depon for mind it a \n",
      "\n",
      "6.2249487915782975\n",
      "[1m 37s (1400 70%) 1.7303]\n",
      "Whes mis and depure to lave in could and to sayless to her very not and wish of accover quite on to al \n",
      "\n",
      "5.42769106054929\n",
      "[1m 44s (1500 75%) 1.7303]\n",
      "Whing mented he prevardieces. Em this knoped though I would name\n",
      "provess of the polesfelde?\"\n",
      "\n",
      "\"Chapper \n",
      "\n",
      "5.351664769248594\n",
      "[1m 51s (1600 80%) 1.7303]\n",
      "Whoss, you do be quite in to say the suply\n",
      "me sackingral and for for as the would be to more open a co \n",
      "\n",
      "5.745141218930193\n",
      "[1m 58s (1700 85%) 1.7303]\n",
      "Whing I have kes not in Annow you saiving the day, as I do not me a glized so not a make I all the\n",
      "wou \n",
      "\n",
      "6.846121249679938\n",
      "[2m 5s (1800 90%) 1.7303]\n",
      "Whing his be said shatious for man it, and till a get disabe gaster\n",
      "convins and for the would we were  \n",
      "\n",
      "6.65928496035587\n",
      "[2m 12s (1900 95%) 1.7303]\n",
      "Whuls a dirry was a bas so the deperturnest be the questing o this\n",
      "resole to be pontrong's such a shar \n",
      "\n",
      "4.385449300978874\n",
      "[2m 19s (2000 100%) 1.7303]\n",
      "Wher I day you pant on famm, I am suppose\n",
      "at moment letter she had point a memore it be is be not in e \n",
      "\n",
      "4.6416038648577596\n",
      "5.893227151266663\n"
     ]
    }
   ],
   "source": [
    "def validate(inp, target):\n",
    "    hidden = decoder.init_hidden()\n",
    "    decoder.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for c in range(chunk_len):\n",
    "        output, hidden = decoder(inp[c], hidden)\n",
    "        loss += criterion(output, target[c].flatten())\n",
    "        \n",
    "    return loss.item()/ chunk_len\n",
    "\n",
    "start = time.time()\n",
    "val_all_losses = []\n",
    "val_loss_avg = 0\n",
    "totPerplexity = 0\n",
    "ppAverage = 0\n",
    "counter = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    val_loss =  validate(*random_validation_set())       \n",
    "    val_loss_avg += val_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "        perplexity = math.exp(val_loss)\n",
    "        print(perplexity)\n",
    "        totPerplexity = totPerplexity + perplexity\n",
    "        counter = counter + 1\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        val_all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "ppAverage = totPerplexity/ counter\n",
    "print(ppAverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0m 6s (100 92%) 1.7303]\n",
      "Whatl very be upprevent. An it\n",
      "pleofbles plep propers been the was of he me shald the Edmund wave of t \n",
      "\n",
      "7.403221795293685\n",
      "7.403221795293685\n"
     ]
    }
   ],
   "source": [
    "val_file = unidecode.unidecode(open('dataset/validation2.txt').read())\n",
    "val_len = len(val_file)\n",
    "n_epochs = 108\n",
    "start = time.time()\n",
    "val_all_losses = []\n",
    "val_loss_avg = 0\n",
    "totPerplexity = 0\n",
    "ppAverage = 0\n",
    "counter = 0\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    val_loss =  validate(*random_validation_set())       \n",
    "    val_loss_avg += val_loss\n",
    "\n",
    "    if epoch % print_every == 0:\n",
    "        print('[%s (%d %d%%) %.4f]' % (time_since(start), epoch, epoch / n_epochs * 100, loss))\n",
    "        print(evaluate('Wh', 100), '\\n')\n",
    "        perplexity = math.exp(val_loss)\n",
    "        print(perplexity)\n",
    "        totPerplexity = totPerplexity + perplexity\n",
    "        counter = counter + 1\n",
    "\n",
    "    if epoch % plot_every == 0:\n",
    "        val_all_losses.append(loss_avg / plot_every)\n",
    "        loss_avg = 0\n",
    "        \n",
    "ppAverage = totPerplexity/ counter\n",
    "print(ppAverage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cake id very, for coather when\n",
      "pointer not do bowfore poosers, and hem indentinought that it in is would a very exprove on the cannot it\n",
      "Cown him in his regreal, as she woord good to here\n",
      "thing it it grav\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Cake', 200, temperature=0.8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower temperatures are less varied, choosing only the more probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ther for a great in the some it it with in the grouse it in the said the never at the present the lance in the said the some in the some it a do not of the said the some it in the said a do not love a g\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=0.2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher temperatures more varied, choosing less probable outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thm grank from his\n",
      "fathide, my esmin! shoust coracqually,\" is, I at I shade it.\"\n",
      "\n",
      "\"not had himsed! He garriak; a with mey, a t; shoubke and said_ formore,\n",
      "pecclectlu, he wournayant you! Hen belily\n",
      "othel\n"
     ]
    }
   ],
   "source": [
    "print(evaluate('Th', 200, temperature=1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "* Train with your own dataset, e.g.\n",
    "    * Text from another author\n",
    "    * Blog posts\n",
    "    * Code\n",
    "* Increase number of layers and network size to get better results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Next**: [Generating Names with a Conditional Character-Level RNN](https://github.com/spro/practical-pytorch/blob/master/conditional-char-rnn/conditional-char-rnn.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
